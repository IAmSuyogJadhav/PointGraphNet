{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from inference import *\n",
    "import glob, os\n",
    "DATA_PATH = '../simulation/data/test_data/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Predictions (if not already generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "ckpt_dir = '../core/static/weights/strategy1_run2_v2_pgn/'\n",
    "# ckpt_dir = '../core/static/weights/strategy2_run3_v2_pgn/'\n",
    "model, params = load_model(ckpt_dir)\n",
    "\n",
    "# Aggregate Data\n",
    "data = glob.glob(os.path.join(DATA_PATH, '*.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best train and validation loss\n",
    "print('Best Train Loss: ', params['history']['train_loss'][-1])\n",
    "print('Best Validation Loss: ', params['history']['val_loss'][-1])\n",
    "\n",
    "# Inference (if not already done)\n",
    "if not os.path.isfile('./preds_strat1.pkl' if params['model']['strategy'] == 1 else './preds_strat2.pkl'):\n",
    "    preds = []\n",
    "    print(\"Performing inference...\")\n",
    "    for d in tqdm(data, leave=True, unit=\"file\"):\n",
    "        d = load_points(d, verbose=False)\n",
    "        dfs = []\n",
    "        for g in tqdm(d, leave=False, unit=\"batch\"):\n",
    "            df = infer(\n",
    "                model, g, params[\"model\"][\"strategy\"], NOISE_THRESH, DEVICE\n",
    "            )\n",
    "            dfs.append(df)\n",
    "\n",
    "        df = pd.concat(dfs)\n",
    "\n",
    "        # # Get rid of nans\n",
    "        # df.dropna(inplace=True)\n",
    "\n",
    "        # Drop duplicates (if any)\n",
    "        df = df.drop_duplicates(subset=[\"x\", \"y\", \"z\"], keep=\"first\")\n",
    "\n",
    "        # Save\n",
    "        preds.append(df)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Predictions\n",
    "import pickle\n",
    "\n",
    "if not os.path.isfile('./preds_strat1.pkl' if params['model']['strategy'] == 1 else './preds_strat2.pkl'):\n",
    "    with open('preds_strat1.pkl' if params[\"model\"][\"strategy\"] == 1 else 'preds_strat2.pkl', 'wb') as f:\n",
    "        pickle.dump(preds, f)\n",
    "else:\n",
    "    preds = pickle.load(open('preds_strat1.pkl', 'rb')) if params[\"model\"][\"strategy\"] == 1 else pickle.load(open('preds_strat2.pkl', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the point clouds\n",
    "gts = [pd.read_parquet(d) for d in data]\n",
    "sum([len(g) for g in gts]), sum([len(p) for p in preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_noise_preds = [df[\"label\"] == NOISE_LABEL for df in preds]\n",
    "is_noise_gts = [df[\"label\"] == 'noise_bg' for df in gts]\n",
    "\n",
    "# true positive, false positive, true negative, false negative\n",
    "tp = [np.sum(p & g) for p, g in zip(is_noise_preds, is_noise_gts)]\n",
    "fp = [np.sum(p & ~g) for p, g in zip(is_noise_preds, is_noise_gts)]\n",
    "tn = [np.sum(~p & ~g) for p, g in zip(is_noise_preds, is_noise_gts)]\n",
    "fn = [np.sum(~p & g) for p, g in zip(is_noise_preds, is_noise_gts)]\n",
    "\n",
    "# Noise prediction precision\n",
    "noise_prec = np.sum(tp) / (np.sum(tp) + np.sum(fp))\n",
    "print('Noise Prediction Precision: ', noise_prec)\n",
    "\n",
    "# Noise prediction recall\n",
    "noise_rec = np.sum(tp) / (np.sum(tp) + np.sum(fn))\n",
    "print('Noise Prediction Recall: ', noise_rec)\n",
    "\n",
    "# Noise prediction F1 score\n",
    "noise_f1 = 2 * np.sum(tp) / (2 * np.sum(tp) + np.sum(fp) + np.sum(fn))\n",
    "print('Noise Prediction F1 Score: ', noise_f1)\n",
    "\n",
    "# Noise prediction accuracy\n",
    "noise_acc = np.mean([np.mean(p.reset_index(drop=True) == g.reset_index(drop=True)) for p, g in zip(is_noise_preds, is_noise_gts)])\n",
    "print('Noise Prediction Accuracy: ', noise_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_errors_noise_sensitive = []\n",
    "angle_errors_noise_agnostic = []\n",
    "for i in range(len(preds)):\n",
    "    noise_pred = is_noise_preds[i]\n",
    "    noise_gt = is_noise_gts[i]\n",
    "    mask = ~noise_pred & ~noise_gt\n",
    "    ang = torch.nn.functional.cosine_similarity(\n",
    "        torch.tensor(preds[i][['nx', 'ny', 'nz']].values[mask]),\n",
    "        torch.tensor(gts[i][['nx', 'ny', 'nz']].values[mask]), dim=1).cpu().numpy()\n",
    "    error = np.arccos(pd.DataFrame(ang).mean()) * 180 / np.pi\n",
    "\n",
    "    # Count dissimilarities in noise_pred and noise_gt, and give them a 90 degree error\n",
    "    count = np.sum(np.logical_xor(noise_pred, noise_gt))\n",
    "    final_error = (error * sum(mask) + count * 90) / (sum(mask) + count)\n",
    "    \n",
    "    angle_errors_noise_sensitive.append(final_error)  # Penalizes noise points\n",
    "    angle_errors_noise_agnostic.append(error)  # Ignores noise points\n",
    "\n",
    "\n",
    "\n",
    "# Angle error in degrees, between the ground truth normal and the predicted normal\n",
    "print('Noise-sensitive angle error: ', np.mean(angle_errors_noise_sensitive))\n",
    "print('Noise-agnostic angle error: ', np.mean(angle_errors_noise_agnostic))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print all the metric in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(\n",
    "    [[noise_prec, noise_rec, noise_f1, noise_acc, np.mean(angle_errors_noise_sensitive), np.mean(angle_errors_noise_agnostic)]],\n",
    "    columns=['Noise Precision', 'Noise Recall', 'Noise F1', 'Noise Accuracy', 'Noise-sensitive Angle Error', 'Noise-agnostic Angle Error'])\n",
    "\n",
    "metrics_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.T.to_csv('metrics_strat1.csv' if params[\"model\"][\"strategy\"] == 1 else 'metrics_strat2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['history']['best_epoch']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
